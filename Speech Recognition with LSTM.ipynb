{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exrateJdQy_D",
        "outputId": "81cc47c1-9df1-4441-9f9d-b85a9c1debcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchaudio.transforms import MelSpectrogram\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# Define the SpeechCommands dataset\n",
        "class SubsetSC(SPEECHCOMMANDS):\n",
        "    def __init__(self, subset: str = None):\n",
        "        super().__init__(\"./\", download=True)\n",
        "\n",
        "        def load_list(filename):\n",
        "            filepath = os.path.join(self._path, filename)\n",
        "            with open(filepath) as fileobj:\n",
        "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
        "\n",
        "        if subset == \"validation\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "        elif subset == \"training\":\n",
        "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
        "            excludes = set(excludes)\n",
        "            self._walker = [w for w in self._walker if w not in excludes]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    # Extract features (waveforms) and labels from the batch\n",
        "    waveforms = [item[0] for item in batch]\n",
        "    labels = [item[2] for item in batch]\n",
        "\n",
        "    # Convert labels to indices\n",
        "    label_indices = [label_to_index(label) for label in labels]\n",
        "\n",
        "    # Pad waveforms to have the same length\n",
        "    max_length = max(waveform.size(1) for waveform in waveforms)\n",
        "    padded_waveforms = torch.stack([F.pad(waveform, (0, max_length - waveform.size(1))) for waveform in waveforms])\n",
        "\n",
        "    return padded_waveforms, torch.tensor(label_indices, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "f7F3dZhdQ4Z6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing split of the data\n",
        "train_set = SubsetSC(\"training\")\n",
        "test_set = SubsetSC(\"testing\")\n",
        "\n",
        "# Batch size, device, and other setup\n",
        "batch_size = 256\n",
        "\n",
        "if device == \"cuda\":\n",
        "    num_workers = 1\n",
        "    pin_memory = True\n",
        "else:\n",
        "    num_workers = 0\n",
        "    pin_memory = False\n",
        "\n",
        "# Create DataLoader instances for training and testing\n",
        "train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,  # Include the collate_fn here\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_fn,  # Include the collate_fn here\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Function to convert labels to indices and vice versa\n",
        "def label_to_index(word):\n",
        "    return labels.index(word)\n",
        "\n",
        "def index_to_label(index):\n",
        "    return labels[index]\n",
        "\n",
        "\n",
        "# Define the RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_classes=None, num_layers=2):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x should have a shape of (batch_size, sequence_length, input_size)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.fc.in_features).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.fc.in_features).to(x.device)\n",
        "        out, _ = self.rnn(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "# Initialize the RNN model, loss function, and optimizer\n",
        "labels = sorted(list(set(datapoint[2] for datapoint in train_set)))\n",
        "num_classes = len(labels)\n",
        "\n",
        "# Modify the input_size argument in the model instantiation\n",
        "model = RNNModel(input_size=16000, hidden_size=64, num_classes=num_classes, num_layers=2)\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4GwN-NxQ9Fq",
        "outputId": "c4148bd3-f7ea-41a0-c8c9-f795cf180f37"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.26G/2.26G [00:28<00:00, 83.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs=5):\n",
        "    total_step = len(train_loader)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for i, (waveform, labels) in enumerate(train_loader):\n",
        "            waveform = waveform.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(waveform)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "# Test function\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for waveform, labels in test_loader:\n",
        "            waveform = waveform.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(waveform)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    print('Test Accuracy: {:.2f}%'.format(accuracy))\n",
        "\n",
        "# Train the RNN model\n",
        "train(model, train_loader, criterion, optimizer, num_epochs=5)\n",
        "\n",
        "# Test the RNN model\n",
        "test(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncFmrUqpRBEM",
        "outputId": "e90bd031-b410-4e27-e4b6-340346b58821"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/332], Loss: 3.4721\n",
            "Epoch [1/5], Step [200/332], Loss: 3.5174\n",
            "Epoch [1/5], Step [300/332], Loss: 3.4296\n",
            "Epoch [2/5], Step [100/332], Loss: 3.2003\n",
            "Epoch [2/5], Step [200/332], Loss: 3.0872\n",
            "Epoch [2/5], Step [300/332], Loss: 3.0874\n",
            "Epoch [3/5], Step [100/332], Loss: 2.8470\n",
            "Epoch [3/5], Step [200/332], Loss: 2.6945\n",
            "Epoch [3/5], Step [300/332], Loss: 2.7981\n",
            "Epoch [4/5], Step [100/332], Loss: 2.3999\n",
            "Epoch [4/5], Step [200/332], Loss: 2.3800\n",
            "Epoch [4/5], Step [300/332], Loss: 2.3871\n",
            "Epoch [5/5], Step [100/332], Loss: 2.0706\n",
            "Epoch [5/5], Step [200/332], Loss: 2.1194\n",
            "Epoch [5/5], Step [300/332], Loss: 2.2249\n",
            "Test Accuracy: 8.27%\n"
          ]
        }
      ]
    }
  ]
}